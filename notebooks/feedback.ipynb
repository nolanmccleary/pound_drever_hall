{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDH Feedback Control\n",
    "\n",
    "This notebook is essentially a paraphrasing of [this document](../Resources/pdh_control.pdf) along with some additional justifications to explain why some things are the way they are. The paper should be used as the validation mechanism if anything written here seems off. To represent imaginary numbers j is used instead of i now because current is now a thing.\n",
    "\n",
    "\n",
    "## 1.1: Basic Concepts\n",
    "\n",
    "Define the following terms:\n",
    "\n",
    "$$\n",
    "\\hat{A}(s) = \\mathcal{L}\\{h(t)\\} = \\frac{Y(s)}{X(s)}\n",
    "$$\n",
    "\n",
    "Where $s = \\sigma + 2\\pi f = \\sigma + j\\omega$. Because we are analyzing the closed-loop behaviour of the system we set $\\sigma$ equal to 0. We can do this because it's a closed loop, so things like an unstable impulse response where a positive $\\sigma$ would cause the system output to become unstable is already encoded in the closed-loop transfer function evaluated along the imaginary axis. In other words, we can work purely in the frequency domain when evaluating the closed-loop behaviour of this system.\n",
    "\n",
    "The gain and phase of $\\hat{A}$ are defined as follows:\n",
    "\n",
    "$$\n",
    "|\\hat{A}|, \\angle{\\hat{A}}\n",
    "$$\n",
    "\n",
    "\n",
    "Loop components perform algebraic operations on their inputs in the frequency domain. These operations correspond to convolutions in the time domain. The open-loop transfer function is the product of all loop components:\n",
    "\n",
    "$$\n",
    "\\alpha = \\hat{K}\\hat{G}\\hat{H}\n",
    "$$\n",
    "\n",
    "In the time domain this represents the end-to-end convolution applied to the input signal. We use a negative feedback loop because we need a way to self-regulate our current response as a function of our priors. Assuming the last output is fed as negative feedback to be subtracted from our input signal before passing through the chain, we can define the closed-loop transfer function as follows:\n",
    "\n",
    "$$\n",
    "e = x - y\\newline\n",
    "y = \\alpha e = \\alpha(x - y)\\newline\n",
    "(1 + \\alpha)y = \\alpha x\\newline\n",
    "\\frac{y}{x} = \\frac{\\alpha}{1 + \\alpha}\n",
    "$$\n",
    "\n",
    "This is super unintuitive unless you treat $y_{o, n}$ and $y_{e, n-1}$ as the same term $y$. We can do this because our system is assumed to be linear time-invariant (LTI). We make this assumption because our transfer function does not change with respect to time. By definition of what the laplace transform does, no frequency domain signals will change with respect to time. This means niether X(s), Y(s), nor H(s) will change with respect to whatever cycle of the loop we are on. It also makes the concept of loop cycles irrelevant because doing this in the frequency domain is inherently equivalent to integrating this recursive feedback loop across all of time. This means that our feedback loop modelled in the laplace domain really just models the convergence of a series in the time domain where each term in that series is a convolution between our transfer function and our input.\n",
    "\n",
    "Let $h(t)$ represent the open-loop impulse response such that a single pass $y_{\\text{open}}(t) = (h * x)(t)$\n",
    "\n",
    "\n",
    "$$\n",
    "y_0(t) = 0\\newline\n",
    "e_1(t) = x(t) - y_0(t) = x(t)\\newline\n",
    "y_1(t) = (h * e_1)(t) = (h * x)(t)\\newline\n",
    "e_2(t) = x(t) - y_1(t)\\newline\n",
    "y_2(t) = (h * e_2)(t) = (h * x)(t) - (h * h * x)(t)\\newline\n",
    "e_3(t) = x(t) - y_2(t)\\newline\n",
    "y_3(t) = (h * x)(t) - (h * h * x)(t) + (h * h * h * x)(t)\\newline\n",
    "$$\n",
    "Edit: There is a more rigorous way to define the above using measure theory such that the series converges to the closed-loop response for $|H(s)| > 1 but I don't understand it so enjoy the ghetto Neumann series instead.\n",
    "\n",
    "\n",
    "Through induction we get the following series:\n",
    "\n",
    "$$\n",
    "y_n(t) = \\sum_{k=1}^n (-1)^{k-1}(h^{*k} * x)(t)\n",
    "$$\n",
    "\n",
    "The steady-state solution is the limit:\n",
    "\n",
    "$$\n",
    "y(t) = \\lim_{n\\rightarrow\\infty}y_n(t) = \\sum_{k=1}^\\infty (-1)^{k-1}(h^{*k} * x)(t)\\newline\n",
    "$$\n",
    "\n",
    "We can also map this series to its Laplace domain counterpart:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}\\{h^{*k}(t)\\} = [H(s)]^k\\newline\n",
    "Y(s) = \\sum_{k=1}^\\infty(-1)^{k-1}H(s)^kX(s) = \\frac{H(s)}{1 + H(s)}X(s)\n",
    "$$\n",
    "\n",
    "The Laplace domain representation gives us a geometric series that converges for all $|H(s)| < 1$. In other words, it will converge as long as the open-loop transfer function does not boost the signal. The frequency domain representation is the sum of the infinite series of convolutions that represent the limiting behaviour of our feedback process. \n",
    "\n",
    "Real-world systems have a delay between the input and the output. We can introduce $\\tau$-shifted delta convolutions into each step of our series to account for this:\n",
    "\n",
    "$$\n",
    "h_\\tau = h(t) * \\delta(t-\\tau)\n",
    "y_n(t) = \\sum_{k=1}^n(-1)^{k-1}(h * h_\\tau^{*(k-1)} * x)(t)\\newline\n",
    "\n",
    "$$\n",
    "\n",
    "Via $\\mathcal{L}\\{\\delta(t - \\tau)\\} = e^{-s\\tau}$\n",
    "\n",
    "$$\n",
    "Y(s) = H(s)X(s)\\sum_{k=0}^{\\infty}(-1)^kH^k(s)e^{-s\\tau k}\n",
    "$$\n",
    "\n",
    "If $|H(s)e^{-s\\tau}| < 1$, this geometric series converges to\n",
    " \n",
    "$$\n",
    "Y(s) = \\frac{H(s)}{1 + H(s)e^{-s\\tau}}X(s)\n",
    "$$\n",
    "\n",
    "The stability of this closed-loop expression is determined by the roots of $1 + H(s)e^{-s\\tau} = 0$.\n",
    "\n",
    "\n",
    "This was a bit off-topic and rather tedious, but I think it's much more powerful and rewarding to think of control theory as a form of applied recursion rather than just memorizing a bunch of block diagram hacks to be implemented through black-box Matlab code.\n",
    "\n",
    "\n",
    "\n",
    "## 1.2: Basic Model\n",
    "\n",
    "Now we have the foundation to build up our control model a little bit. This is what it looks like:\n",
    "\n",
    "#### Figure 1.2: System Diagram and Bode Plots\n",
    "<p align=\"left\">\n",
    "  <img src=\"../images/control_1.png\" width=\"500\">\n",
    "</p>\n",
    "\n",
    "In the context of figure b we get:\n",
    "\n",
    "$$\n",
    "\\alpha = KGH\n",
    "$$\n",
    "$$\n",
    "\\frac{y_5}{m_6} = \\frac{\\alpha}{1 + \\alpha}\n",
    "$$\n",
    "$$\n",
    "y_1 = \\frac{\\alpha}{1 + \\alpha}(\\frac{m_6}{H} + \\frac{n_1}{\\alpha} - \\frac{n_5}{H})\n",
    "$$\n",
    "\n",
    "We can correlate a signal with a delayed version of itself to figure out how much power the signal shares with a time-shifted version of itself. This is called autocorrelation. We can take the Fourier transform of that autocorrelation across all given time offsets to get the power spectral density (PSD) function which tells us how much of the signal's power is being dispersed at any given frequency.\n",
    "\n",
    "$$\n",
    "S_{xx}(f) = \\int_{-\\infty}^\\infty R_{xx}(\\tau)e^{-j2\\pi f\\tau}d\\tau\n",
    "$$\n",
    "\n",
    "Where\n",
    "$$\n",
    "R_{xx}(\\tau) = \\lim_{T\\rightarrow\\infty}\\frac{1}{T}\\int_0^Tx(t)x(t + \\tau)dt\n",
    "$$\n",
    "\n",
    "Then \n",
    "$$\n",
    "S_{y1} = \\frac{1}{|1 + \\alpha|^2}S_{n1} + \\frac{|\\alpha|^2}{|1 + \\alpha|^2}\\frac{1}{|H|^2}S_{n5}\n",
    "$$\n",
    "\n",
    "Ideal negative feedback is defined by unity closed-loop gain at frequencies of interest i.e. \n",
    "\n",
    "$$\n",
    "|\\alpha(j\\omega)| \\rightarrow \\infty\n",
    "$$\n",
    "$$\n",
    "\\angle \\alpha(j\\omega) = 0\\newline\n",
    "$$\n",
    "$$\n",
    "\\frac{\\alpha}{1 + \\alpha} \\rightarrow 1\n",
    "$$\n",
    "\n",
    "This implicitly means that noise from $n_1$ is completely eliminated. However, sensor noise from $n_5$ is still present:\n",
    "$$\n",
    "S_{y1} \\rightarrow \\frac{1}{|H|^2}S_{n5}\n",
    "$$\n",
    "\n",
    "From the above, we see that high gains at frequencies of interest will suppress laser noise but at the expense of increased susceptibility to sensor noise. Intuitively, as we increase the gain we are better able to detect laser noise, and thus better able to apply feedback to correct it. However, as our gain increases, so does the amount of noise introduced by the sensor because it is getting boosted by the closed-loop gain as well, and we are not able to correct it with feedback. This means that we need to consider the tradeoff between plant (laser) and sensor noise during the design process.\n",
    "\n",
    "As per standard convention, we define the unity gain as the frequency where open-loop gain is equal to one ($|\\alpha(f_{UG})| = 1 = 0dB$) and the phase margin as $\\phi_m = \\pi + \\angle\\alpha(f_{UG})$. Phase margin tells us how much room we have at unity gain before our system becomes unstable. Given the characteristic equation:\n",
    "\n",
    "$$\n",
    "1 + \\alpha = 0\n",
    "$$\n",
    "$$\n",
    "|\\alpha| = 1\n",
    "$$\n",
    "\n",
    "It's easy to see $\\angle\\alpha = \\pi$ at unity gain would cause a pole. Phase margin is simply a way to quantify how far we are from this pole. We can also do the inverse of this by getting the frequency where phase is equal to pi, then figuring out the gain margin by dividing 1 by the gain at that frequency i.e. $\\angle \\alpha(f_\\pi) = -\\pi \\text{ and } g_m = \\frac{1}{|\\alpha(f_\\pi)|}$. \n",
    "\n",
    "Let $e = x - y = x - \\alpha e \\rightarrow \\frac{e}{x} = \\frac{1}{1 + \\alpha}$. Around unity gain we see a bump in this error response because this is when the magnitude of the error response denominator tends to be smallest. We define $f_{bump} = \\argmax_\\omega|\\frac{E(j\\omega)}{X(j\\omega)}| = \\argmax_\\omega|\\frac{1}{1 + \\alpha}|$. This maps to the frequency with the highest PSD on the error response's spectrum and it represents the frequency at which the system is worst at tracking disturbances. \n",
    "\n",
    "Controllers are made to minimize error, the point where the error response is highest is the point where the controller is doing the worst job over the course of the recursion's life cycle at minimizing error. In other words, modes within the loop operating at this frequency will contribute disproportionately to the overall error present throughout controller's operation.\n",
    "\n",
    "What I mean by this is that a control loop is a recursion. Modes of different frequencies propagate through that recursion with respect to time. Error accumulates with respect to time as the controller is operating. Transforming to the frequency domain collapses that recursion. Modes oscillating at the servo bump frequency contribute to the overall recursion's lifetime error the most. \n",
    "\n",
    "System stability can be throught of as the interference patterns between modes in the system being bounded such that there exists an upper limit on their magnitude that is within the system's tolerances. As a result of asserting this bound, there is virtually never a good time to use positive feedback in a control loop as recursion with positive feedback yields to unbounded behaviour. This generally results in either damage or saturation of the system, in both cases the response tends to be highly inappropriate. This intuition is formalized through the concepts of BIBO stability and the Nyquist criterion. [Lyapunov Stability](https://www.cds.caltech.edu/~murray/courses/cds101/fa02/caltech/mls93-lyap.pdf) generalizes this further by defining a generic energy function from which to evalutate the system's stability.\n",
    "\n",
    "We showed the loop above but will now solidify the context. We define our plant as $G(f)$, this represents our laser. Our sensor is represented as $H(f)$. This gives the Laplace domain representation of our error signal with respect to the laser output as per [the preceding notebook](../notebooks/pound_drever_hall.ipynb). This assumes that we have a Laplace model for our laser. If we don't then we should seek to find the black-box relationship between the error signal and the laser driver current instead. Assume that H includes the mixer and LPF as discussed previously. Analog PID control can be implemented by a loop filter where it's transfer function can be written as:\n",
    "\n",
    "$$\n",
    "K = K_p(1 + \\frac{\\omega_I}{j\\omega} + j\\frac{\\omega}{\\omega_D}) = K_p(1 - j\\frac{\\omega_I}{\\omega} + j\\frac{\\omega}{\\omega_D})\n",
    "$$\n",
    "\n",
    "Where $\\omega_I$ and $\\omega_D$ are the integral and derivative corner frequencies respectively. The reason why $j\\omega$ maps to differentiation in the time domain or $\\frac{-j}{\\omega}$ maps to integration in the time domain is because when they are applied to a signal in the frequency domain, their representation is equivalent to if that frequency signal's native time-domain counterpart was transformed with either a differentiated or integrated Fourier operator respectively. By integration by parts, we can see that this is in turn equivalent to a standard Fourier operator transforming the differentiated or integrated signal (respectively) instead of the native signal.\n",
    "\n",
    "\n",
    "\n",
    "## 1.3: Convergence Game\n",
    "\n",
    "The main idea behind everything we are doing here is that we are building something to set the state of some given system to target state as quickly and as predictably as possible. We do this by using a feedback loop. This loop is inherently recursive, and the convergence of this recursion is the convergence of our actual state with our target state. If the recursion diverges so does our state. We use frequency domain analysis so we can collapse the recursion and reason about it algebraically. We then bring in frequency-domain representations of each part of our system to do this. Once we have a closed-loop representation of our system, we can get to work mapping the frequency spectra of all possible input states. This representation, along with the frequency representations of the injected noise sources, allows us to model all modes that will oscillate throughout our recursion. We can then perform stability analysis both algebraically and numerically to assess how our system responds to these oscillations. We can measure how effectively our system converges by how small the error response is. The smaller the error response, the less overall error in the system and the smaller the difference between the curves of the target and the actual state plotted in the time domain. We can tweak our system by changing the physical things in it that impact the transfer function of each block in our feedback loop e.g. resistors, capacitors etc. By doing this, we are able to implement recursion in a useful manner through a physical system. This is reasonably cool.\n",
    "\n",
    "In summary:\n",
    "1. Unity gain is when our open loop gain $\\alpha = KGH$ has a magnitude equal to 1. We want to keep track of our phase at this point so our closed loop response $\\frac{\\alpha}{1 + \\alpha}$, proven by the series convergence above doesn't blow up. \n",
    "\n",
    "2. Ideal negative feedback is when $\\alpha \\rightarrow \\infty, \\angle\\alpha = 0$ at all frequencies of interest (across the bandwidth of our system). When this happens, our closed-loop response is equal to 1. By the closed loop response being equal to 1, we know that the series:\n",
    "\n",
    "$$\n",
    "h_n(t) = \\sum_{k=1}^n(-1)^{k-1}(\\alpha * \\alpha_\\tau^{*(k-1)})(t)\\newline\n",
    "$$\n",
    "\n",
    "Converges to $\\delta(t)$ in the time domain. This means that in the time domain applying this transfer function to any x(t) would give us x(t). If we define an identity mapping such that any x(t) has some ideal target z(t) that is bijective with x(t), it implies that convolution with our closed-loop transfer function would preserve this mapping perfectly. If we define x(t) to instead be our target, then this mapping is trivial. In both cases the implication is that with an ideal negative impulse response we can reach our target value perfectly. In figure 1.2, $m_6$ represents the target. The key performance goal of the system is to get $\\frac{y_5}{m_6}$ to as close to 1 at all loop frequencies of interest as possible. What this would mean is that all across our error signal's frequency spectrum (all combinations of harmonics that may be present in the error signal) our detected laser frequency $y_5$ perfectly converges on our target frequency $m_6$ to the full extent that we are able to compare the two. From this, we can also intuitively understand the servo bump frequency as the frequency where our response is least convergent with our target.\n",
    "\n",
    "\n",
    "Generally speaking, there are three key ways to make the system more effective:\n",
    "\n",
    "1. Higher performance (Lower error)\n",
    "\n",
    "2. Higher bandwidth (Faster speed)\n",
    "\n",
    "3. Higher robustness (More stable)\n",
    "\n",
    "Higher performance is usually assesed by the magnitude characteristics of the error response. Higher bandwidth means that the feedback loop is better able to track fast-changing signals. Higher robustness means the system is less likely to become unstable. We can generally optimize pretty well for two of these things at the same time but we sacrifice the third. As such, it makes the most sense to define a minimal baseline level of robustness and then optimize everything else from there. This is what we will now do:\n",
    "\n",
    "1. Maximize $f_{UG}$. Unity gain is a stability risk and also a good landmark for where the 'weak feedback' zone lies. In other words, it marks the boundary between good control and poor control. By making this as large as possible, we increase the effective bandwidth of our system allowing us to better use higher frequency components in our system. \n",
    "\n",
    "2. Maintain $\\frac{\\pi}{6} < \\phi_m < \\frac{\\pi}{3}$ i.e. add enough delay so that the open-loop gain is wound back at least this far counterclockwise at unity magnitude. A better margin would be nice for more stabillity but it comes at the cost of a slower system. Remember, $\\mathcal{F}\\{\\delta(t-\\tau) = e^{-j\\omega\\tau}\\}$. Every time we wind our response's phase back in the frequency domain we impart a delay in the time domain. You can also think of this as pulses travelling around a loop. If the loop is too fast, the next pulse will reach a node where the last pulse still hasn't finished dissipating. If this happens the pulses will stack, and then that pulse will be fed through the loop again back onto itself, which still hasn't finished dissipating. This is in essense positive feedback. And it's described in the frequency domain by a phase shift of $\\pi$. However, it's a lot more natural to think about it in the time domain first before trying to come to terms with the frequency domain expression for it.\n",
    "\n",
    "3. For $f < f_{UG}$, maintain $\\angle\\alpha > -\\frac{2}{3}\\pi$. This gives us a nice stability margin to ensure no poles are present in our active range.\n",
    "\n",
    "\n",
    "\n",
    "## 1.4: Laser Noise\n",
    "\n",
    "We can model laser noise as a combination of Gaussian white noise and flicker noise, which can be modelled as Gaussian noise where it's expected power output per unit bandwidth is equal to the baseline expectation at 1 Hz divided by the frequency. In the time domain we can visualize this as \n",
    "$$\n",
    "n(t) = A\\,h_w(t) + n_f(t)\n",
    "$$\n",
    "where $h_w(t)$ is a white Gaussian noise process and $n_f(t)$ is a random process chosen so that its power spectral density (PSD) is proportional to $1/f$ over the measurement band.\n",
    "\n",
    "Correlating $A h_w(t)$ gives us\n",
    "$$\n",
    "R_{A h_w}(\\tau) = A^{2}\\,\\delta(\\tau)\n",
    "$$\n",
    "which is what we'd expect (the noise correlates with itself perfectly at that exact moment and at no other time because it's completely random). For the flicker part, we choose $n_f(t)$ so that when we take the Fourier transform of its autocorrelation, the expected power dissipated at any given frequency is equal to the baseline expectation divided by the frequency. In short, we need the flicker component to be whatever function that gives us this shape. This will require that the flicker component itself be a random process. 'Proof':\n",
    "\n",
    "If we instead tried to let a deterministic envelope multiply white noise, the autocorrelation collapses to a delta and does not produce a $1/f$ spectrum, so that construction won’t work. Hence we directly construct a random process with the target spectrum.\n",
    "\n",
    "When we take the PSD of the correlation of our properly formed time domain function, we get the following:\n",
    "$$\n",
    "S_{n}(f) = \\frac{h_{-1}}{f} + h_{0}\\qquad (f_L \\le f \\le f_H)\n",
    "$$\n",
    "$h_{0}$ and $h_{-1}$ are constants because white Gaussian noise processes distribute power evenly across all frequencies and the flicker component contributes a $1/f$ term within the measurement band. In our PSD representation, the combination of these two terms gives us a 1-Hz frequency intercept of $h_{0}$ and a decaying hyperbolic on the left-hand side. Connecting back to the laser noise, [we see here](./physics_as_needed.ipynb) that when we add gaussian white noise to the laser the Lorentzian bump widens, if we add flicker noise it will become less smooth. The intution is as follows:\n",
    "\n",
    "1. White noise is fundamentally just adding entropy to our system. If the PSD of all frequencies were the exact same (like with white noise), then our signal would not be able to convey any meaningful information. Our Lorentzian spike has structure, and the more white noise that is added, the less structure it is able to maintain. However, since white noise itself is uniform, this effect is felt uniformly across the spike.\n",
    "\n",
    "2. Flicker noise is a comically dressed villain who loudly proclaims \"I will change your laser's frequency for a relatively long period of time and mess up your frequency spectrum in the process\".\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022a0bc00ad74b5ba8f863b220de1668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatLogSlider(value=0.001, description='h0 (white)', max=0.0, min=-6.0), FloatLogSlider…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, FloatLogSlider\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"figure.figsize\": (8, 5),\n",
    "    \"figure.dpi\": 140,\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"axes.titlesize\": 13,\n",
    "    \"xtick.labelsize\": 11,\n",
    "    \"ytick.labelsize\": 11,\n",
    "    \"lines.linewidth\": 2.2,\n",
    "})\n",
    "\n",
    "f_line = np.linspace(-8.0, 8.0, 5001)     \n",
    "f_psd  = np.logspace(-2, 3, 2000)       \n",
    "\n",
    "\n",
    "gamma_base = 0.15        \n",
    "f0_tail    = 0.15        \n",
    "A_white_to_width   = 30\n",
    "A_flicker_to_wings = 4.0\n",
    "\n",
    "def lorentzian(f, gamma):\n",
    "    return (gamma/np.pi) / (f**2 + gamma**2)\n",
    "\n",
    "def flicker_wings(f):\n",
    "    return 1.0 / (np.abs(f) + f0_tail)\n",
    "\n",
    "def make_line(h0, h1):\n",
    "    gamma = gamma_base * (1.0 + A_white_to_width * np.sqrt(h0))\n",
    "    core  = lorentzian(f_line, gamma)\n",
    "    wings = A_flicker_to_wings * h1 * flicker_wings(f_line)\n",
    "    line  = core + wings\n",
    "    area = np.trapz(line, f_line)\n",
    "    return line/area if area > 0 else line\n",
    "\n",
    "def draw(h0=1e-3, h_1=1e-3):\n",
    "    line_white_only   = make_line(h0, 0.0)\n",
    "    line_flicker_only = make_line(0.0, h_1)\n",
    "    line_combo        = make_line(h0, h_1)\n",
    "\n",
    "    S_white   = np.full_like(f_psd, fill_value=h0)     \n",
    "    S_flicker = np.maximum(h_1 / f_psd, 1e-18)       \n",
    "    S_combo   = S_white + S_flicker\n",
    "\n",
    "    plt.figure()\n",
    "    plt.semilogy(f_line, np.maximum(line_white_only,   1e-12), label=\"white only\")\n",
    "    plt.semilogy(f_line, np.maximum(line_flicker_only, 1e-12), label=\"flicker only\")\n",
    "    plt.semilogy(f_line, np.maximum(line_combo,        1e-12), label=\"combined\")\n",
    "    plt.title(\"Optical Line Shape (area-normalized)\")\n",
    "    plt.xlabel(\"Frequency offset (arb. units)\")\n",
    "    plt.ylabel(\"Spectral density (arb. units)\")\n",
    "    plt.xlim(-4.0, 4.0)\n",
    "    plt.ylim(1e-5, 2.0)\n",
    "    plt.grid(True, which=\"both\", alpha=0.35)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.loglog(f_psd, S_white,   linestyle=\"--\", label=r\"$h_0$ (white)\")\n",
    "    plt.loglog(f_psd, S_flicker, linestyle=\"--\", label=r\"$h_{-1}/f$ (flicker)\")\n",
    "    plt.loglog(f_psd, S_combo,               label=\"combined\")\n",
    "    plt.title(r\"Noise PSD: $S_n(f)=h_0+h_{-1}/f$\")\n",
    "    plt.xlabel(\"Frequency (arb. units)\")\n",
    "    plt.ylabel(\"PSD (arb. units)\")\n",
    "    plt.ylim(1e-6, 1e2)\n",
    "    plt.grid(True, which=\"both\", alpha=0.35)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    print(f\"h0 (white): {h0:.2e}   h-1 (flicker): {h_1:.2e}\")\n",
    "\n",
    "\n",
    "h0_slider  = FloatLogSlider(value=1e-3, base=10, min=-6, max=0, step=0.1, description='h0 (white)')\n",
    "h1_slider  = FloatLogSlider(value=1e-3, base=10, min=-6, max=0, step=0.1, description='h-1 (flicker)')\n",
    "\n",
    "_ = interact(draw, h0=h0_slider, h_1=h1_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flicker noise 'squeezes' the sides of the spike inward and makes the top more pointy because small frequency deviations of relatively high duration are statistically more likely than large frequency deviations of relatively high duration. Since we are taking the Fourier transform of an expectation we would expect those statistical properties to carry over. \n",
    "\n",
    "\n",
    "## 1.5: Noise Sensitivity\n",
    "\n",
    "Given our diagram, we can model the transfer function of our laser output $y_1$ with respect to laser noise $n_1$ as follows:\n",
    "\n",
    "$$\n",
    "y_1 = n_1 - \\alpha y_1\n",
    "$$\n",
    "$$\n",
    "y_1 = \\frac{n_1*1}{1+\\alpha}\n",
    "$$\n",
    "$$\n",
    "\\frac{y_1}{n_1} = \\frac{1}{1+\\alpha}\n",
    "$$\n",
    "\n",
    "This transfer function represents our closed loop's sensitivity to laser noise. It measures to what degree our laser output is affected by the noise value when our loop is closed. For ideal negative feedback this number is zero meaning the noise is completely rejected.\n",
    "\n",
    "We can do the same thing for sensor noise:\n",
    "$$\n",
    "y_1 = -\\alpha y_1 - \\frac{\\alpha}{H}n_5\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{y_1}{n_5} = -\\frac{\\alpha}{H(1+\\alpha)}\n",
    "$$\n",
    "\n",
    "This transfer function represents our closed loop's sensitivity to sensor noise. It measures the degree to which our laser's output will be affected by the sensor noise. At ideal negative feedback, we see how while laser noise sensitivity approaches zero, sensor noise sensitivity will approach $-\\frac{1}{H}$. This is the laser-sensor noise tradeoff previously discussed.\n",
    "\n",
    "\n",
    "\n",
    "## 1.6: Tuning Procedure\n",
    "\n",
    "We can now describe the tuning procedure at a high level:\n",
    "\n",
    "\n",
    "1. Make sure $\\alpha$ is sufficiently large across our target bandwidth. Do this by adding a sinusoidal probe to our DC setpoint value (i.e. test small deviations from setpoint at all frequencies in the error signal's spectrum). Doing this is representative of seeing what our open-loop gain is at any harmonic that is possibly contained inside our error signal. This is also known as a small-signal sweep.\n",
    "2. Assess stability (unity gain, phase margins etc).\n",
    "3. Substitute alpha into our closed and open-loop noise sensitivity transfer functions. We want the closed-loop to reject as much noise as possible relative to the open loop. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
